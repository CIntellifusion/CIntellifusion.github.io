---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I received my Bachelor's degree from Renmin University of China, where I worked under the supervision of Prof. Jun He from Renmin University of China and Prof. Hongyan Liu from Tsinghua University. During my visit to HKUST, I was fortunate to be advised by Prof. Qifeng Chen, focusing on video generation. After that, I was an intern at MSRA, working with Dr. Junliang Guo and Tianyu He focusing on video generation and world simulator.

My research interests lie in diffusion models, video generation, and world modeling. I am currently interested in build interactive, real-time, and consistent video generation models that can serve as world simulators.

<!-- <span style="color: red;">I am actively seeking PhD opportunities for Spring/Fall 2026.</span>  -->
I am a highly self-motivated student with a deep passion for research and coding. I am eager to work on a series of meaningful projects to advance video generation as a foundation for world simulators.

<!-- You can find my [CV](https://github.com/CIntellifusion/CIntellifusion.github.io/blob/main/Haoyu_En_CV%20.pdf) here.  -->

# ğŸ”¥ News
- *2025.09.22*: &nbsp;ğŸ‰ğŸ‰ [Geometry Forcing](https://geometryforcing.github.io/) is accepted to [NeurIPS 2025 NextVid Workshop](https://what-makes-good-video.github.io/)!
- *2025.07.11*: &nbsp;ğŸ‰ğŸ‰ We release [Geometry Forcing](https://geometryforcing.github.io/)!
- *2025.02.26*: &nbsp;ğŸ‰ğŸ‰ VideoDPO was accepted by CVPR2025!
- *2024.12.19*: &nbsp;ğŸ‰ğŸ‰ We make the [VideoDPO](https://videodpo.github.io) paper and code public!
- *2024.11.01*: &nbsp;ğŸ‰ğŸ‰ We make the [VideoTuna](https://github.com/VideoVerses/VideoTuna) V0.1.0 public!
- *2023.07*: &nbsp;ğŸ‰ğŸ‰ Emotalk is accepted by ICCV23.

# ğŸ“ Publications 

<!-- videodpo  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2025</div><img src='images/geometry_forcing.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling](https://geometryforcing.github.io/)

**Haoyu Wu\***, Diankun Wu\*, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, Jiang Bian

[**Paper**](https://arxiv.org/abs/2507.07982) [**Project**](https://geometryforcing.github.io/) [**Code**](https://github.com/CIntellifusion/GeometryForcing)
- Geometry Forcing encourages video diffusion models to internalize latent 3D representations in order to bridge the gap between video diffusion models and the 3D nature of the real world. 
</div>
</div>


<!-- videodpo  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/videodpo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VideoDPO: Omni-Preference Alignment for Video Diffusion Generation](https://videodpo.github.io/)

Runtao Liu\*, **Haoyu Wu\***, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, Qifeng Chen

[**Paper**](https://arxiv.org/abs/2412.14167) [**Project**](https://videodpo.github.io) [**Code**](https://github.com/CIntellifusion/VideoDPO) 
- We propose a whole pipeline for DPO finetuning video diffusion models.
</div>
</div>

<!-- emotalk  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/emotalk.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation](https://ziqiaopeng.github.io/emotalk)

Ziqiao Peng, **Haoyu Wu**, Zhenbo Song, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, Zhaoxin Fan
[**Project**](https://ziqiaopeng.github.io/emotalk/) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose an end-to-end neural network for speech-driven emotion-enhanced 3D facial animation.
</div>
</div>



- [[preprint] VGG-Tex: A Vivid Geometry-Guided Facial Texture Estimation Model for High Fidelity Monocular 3D Face Reconstruction](https://arxiv.org/abs/2409.09740)
    **Haoyu Wu**, Ziqiao Peng, Xukun Zhou, Yunfei Cheng, Jun He, Hongyan Liu, Zhaoxin Fan

# ğŸ“– Educations
- *2021.09 - 2025.07*, Undergraduate student at Renmin University of China, Beijing, China.
- *2024.07 - 2025.01*, Visiting student supervised by Prof. Qifeng Chen at HKUST, Hong Kong, China.

# ğŸ’» Internships

- *2024.11-2025.07*, ML Group, Microsoft Research Asia

# ğŸ“• Teaching Experiences
<!-- - *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->
- *2024.09 - 2025.01*, Teaching Assistant of Introduction to Computer System (I), Renmin University of China.

# ğŸ’¬ Invited Talks
- *2023.01*, "Introduction to Linux" of â€Missing Classesâ€ series in RUC Computer Association
- *2023.08*, AITIME Debate about 3D digital human development \| [\[video\]](https://www.bilibili.com/video/BV1Xh4y1F7Ec/)

# ğŸ– Honors and Awards
- *2022.11* The Chinese Mathematical Competition,First Prize

