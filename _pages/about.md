---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a fourth-yead undergraduate student at Renmin Unversity of China, supervised by Prof. Jun He from Renmin University of China and Prof. Hongyan Liu from Tsinghua University.

My research interest lies in generative models, AI-generated content, video generation, talking head generation. 

I am seeking PhD opportunities for the fall of 2025.

My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).


# ğŸ”¥ News
- *2023.07*: &nbsp;ğŸ‰ğŸ‰ Emotalk is accepted by ICCV23.

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[[ICCV 2023] EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation](https://ziqiaopeng.github.io/emotalk)

Ziqiao Peng, **Haoyu Wu**, Zhenbo Song, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, Zhaoxin Fan
[**Project**](https://ziqiaopeng.github.io/emotalk/) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose an end-to-end neural network for speech-driven emotion-enhanced 3D facial animation.
</div>
</div>

- [[preprint] VGG-Tex: A Vivid Geometry-Guided Facial Texture Estimation Model for High Fidelity Monocular 3D Face Reconstruction](https://arxiv.org/abs/2409.09740)

**Haoyu Wu**, Ziqiao Peng, Xukun Zhou, Yunfei Cheng, Jun He, Hongyan Liu, Zhaoxin Fan


# ğŸ– Honors and Awards
- *2022.11* The Chinese Mathematical Competition,First Prize


# ğŸ“– Educations
- *2021.09 - 2025.07 (now)*, Undergraduate student at Renmin University of China, Beijing, China.
- *2024.07 - 2025.01 (now)*, Visiting student supervised by Prof. Qifeng Chen at HKUST, Hong Kong, China.

# ğŸ’¬ Invited Talks
- *2023.01*, "Introduction to Linux" of â€Missing Classesâ€ series in RUC Computer Association
- *2023.08*, AITIME Debate about 3D digital human development \| [\[video\]](https://www.bilibili.com/video/BV1Xh4y1F7Ec/)

# Teaching Experiences
- *2024.09 - 2025.01*, Teaching Assistant of Introduction to Computer System (I), Renmin University of China.

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.