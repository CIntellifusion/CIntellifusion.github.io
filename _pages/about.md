---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a fourth-year undergraduate student at Renmin University of China, working under the supervision of Prof. Jun He (Renmin University) and Prof. Hongyan Liu (Tsinghua University). During my visit to HKUST, I was mentored by Prof. Qifeng Chen.

My research interests include generative models, AI-generated content, video generation, and talking head generation. I also keep up with a wide range of research in AI, having explored papers on topics such as MLLM, diffusion models, PEFT, and RLHF, which I have saved [here](https://awesome-papers-vercel.vercel.app/) and [here](https://github.com/CIntellifusion/Awesome-Papers).

I am actively **seeking PhD opportunities** starting in Fall 2025. If my work aligns with your interests, please feel free to reach out to me. I am highly self-motivated and robust. 


# ğŸ”¥ News
- *2023.07*: &nbsp;ğŸ‰ğŸ‰ Emotalk is accepted by ICCV23.

# ğŸ“ Publications 

<!-- emotalk  -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/emotalk.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation](https://ziqiaopeng.github.io/emotalk)

Ziqiao Peng, **Haoyu Wu**, Zhenbo Song, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, Zhaoxin Fan
[**Project**](https://ziqiaopeng.github.io/emotalk/) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose an end-to-end neural network for speech-driven emotion-enhanced 3D facial animation.
</div>
</div>

- [[preprint] VGG-Tex: A Vivid Geometry-Guided Facial Texture Estimation Model for High Fidelity Monocular 3D Face Reconstruction](https://arxiv.org/abs/2409.09740)
    **Haoyu Wu**, Ziqiao Peng, Xukun Zhou, Yunfei Cheng, Jun He, Hongyan Liu, Zhaoxin Fan


# ğŸ– Honors and Awards
- *2022.11* The Chinese Mathematical Competition,First Prize


# ğŸ“– Educations
- *2021.09 - 2025.07 (now)*, Undergraduate student at Renmin University of China, Beijing, China.
- *2024.07 - 2025.01 (now)*, Visiting student supervised by Prof. Qifeng Chen at HKUST, Hong Kong, China.

# ğŸ’¬ Invited Talks
- *2023.01*, "Introduction to Linux" of â€Missing Classesâ€ series in RUC Computer Association
- *2023.08*, AITIME Debate about 3D digital human development \| [\[video\]](https://www.bilibili.com/video/BV1Xh4y1F7Ec/)

<!-- # Teaching Experiences -->

# ğŸ’» Teaching Experiences
<!-- - *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->
- *2024.09 - 2025.01*, Teaching Assistant of Introduction to Computer System (I), Renmin University of China.